{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from synthetic_env.Env import FiniteStateFiniteActionMDP\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import argparse\n",
    "from gridword_env.gridworld import GridWorld\n",
    "import concurrent.futures\n",
    "from copy import deepcopy\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Params\n",
    "np.random.seed(25)\n",
    "env = 'synthtetic' #choose between \"synthetic\", \"gridword\" and \"garnett\"\n",
    "N = 5 #number of agengts\n",
    "ep = 0.9 #Hetreogeinty transition kernels\n",
    "er = 0.0 #Hetreogeinty rewards\n",
    "S, A = 3, 3 # state space\n",
    "gamma = 0.95 #discount factor \n",
    "init_dist = np.full(S, 1/S) #init_distribution\n",
    "number_rounds = 5000\n",
    "Local_steps = 10\n",
    "step_size  = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate environnements\n",
    "env = FiniteStateFiniteActionMDP(S=S, A=A)\n",
    "common_P = env.get_P()\n",
    "common_r = env.get_r()\n",
    "envs = [FiniteStateFiniteActionMDP( S=S, A=A, epsilon_p=ep, epsilon_r = er, common_transition=common_P, common_reward=common_r) for _ in range(N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Useful functions\n",
    "\n",
    "def compute_mrp_transition(agent, policy):\n",
    "    transition_kernel =  envs[agent].get_P()\n",
    "    mrp_transition = np.sum(policy[:, :, np.newaxis] * transition_kernel, axis=1)\n",
    "    return mrp_transition\n",
    "\n",
    "def compute_mrp_reward(agent, policy):\n",
    "    reward = envs[agent].get_r()\n",
    "    # Element-wise multiplication and sum along the actions axis\n",
    "    mrp_reward = np.sum(policy * reward, axis=1)\n",
    "    return mrp_reward\n",
    "\n",
    "def compute_stationnary_distribution(agent, policy):\n",
    "    mrp_transition = compute_mrp_transition(agent, policy)\n",
    "    stationnary_distribution = (1- gamma) * init_dist.T @ np.linalg.inv(np.eye(S) -gamma *mrp_transition)\n",
    "    return stationnary_distribution\n",
    "\n",
    "def compute_value_function(agent, policy):\n",
    "    mrp_transition = compute_mrp_transition(agent, policy)\n",
    "    mrp_reward = compute_mrp_reward(agent, policy)\n",
    "    return np.linalg.inv(np.eye(S) -gamma *mrp_transition) @ mrp_reward\n",
    "\n",
    "def compute_qfunction(agent, policy):\n",
    "    reward = envs[agent].get_r()\n",
    "    transitions = envs[agent].get_P()\n",
    "    value_function = compute_value_function(agent, policy)\n",
    "    expected_future_rewards = np.sum(transitions * value_function[np.newaxis, np.newaxis, :], axis=2)\n",
    "    Q_function = reward + gamma * expected_future_rewards\n",
    "    return Q_function\n",
    "\n",
    "def compute_policy(logits):\n",
    "    logits_max = np.max(logits, axis=1, keepdims=True)\n",
    "    exp_logits = np.exp(logits - logits_max)\n",
    "    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "def compute_gradient(agent, logits):\n",
    "    policy = compute_policy(logits)\n",
    "    stationary_distribution = compute_stationnary_distribution(agent, policy)\n",
    "    stationary_distribution = stationary_distribution[:, np.newaxis]\n",
    "    q_function = compute_qfunction(agent, policy)\n",
    "    value_function = compute_value_function(agent, policy)\n",
    "    advantage = q_function - value_function[:, None]\n",
    "    gradient = (1 / (1 - gamma)) * stationary_distribution * policy * advantage\n",
    "    return gradient\n",
    "\n",
    "def compute_objective(logits):\n",
    "    policy = compute_policy(logits)\n",
    "    objective = 0.0\n",
    "    for agent in range(N):\n",
    "        statinnary_distrubtion_agent = compute_stationnary_distribution(agent, policy)\n",
    "        reward_mrp = compute_mrp_reward(agent, policy)\n",
    "        objective += np.dot(statinnary_distrubtion_agent,reward_mrp)\n",
    "    return(objective/N)\n",
    "\n",
    "def compute_objective_agent_i(logits,agent):\n",
    "    policy = compute_policy(logits)\n",
    "    statinnary_distrubtion_agent = compute_stationnary_distribution(agent, policy)\n",
    "    reward_mrp = compute_mrp_reward(agent, policy)\n",
    "    objective = np.dot(statinnary_distrubtion_agent,reward_mrp)\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7758204927854486\n",
      "[[9.99976699e-01 1.00533140e-05 1.32476896e-05]\n",
      " [1.64007207e-06 9.99989762e-01 8.59748588e-06]\n",
      " [2.03377798e-05 8.69108086e-06 9.99970971e-01]]\n",
      "0.7758219773520033\n",
      "[[9.99983886e-01 1.11753113e-05 4.93915664e-06]\n",
      " [2.18266305e-06 9.99994262e-01 3.55550607e-06]\n",
      " [2.01922177e-05 7.88618967e-06 9.99971922e-01]]\n",
      "0.7758211548204447\n",
      "[[9.99975882e-01 7.48825900e-06 1.66296242e-05]\n",
      " [1.59198847e-06 9.99992685e-01 5.72323769e-06]\n",
      " [1.51079749e-05 9.29436337e-06 9.99975598e-01]]\n",
      "0.7758213042761619\n",
      "[[9.99977758e-01 8.74006426e-06 1.35020928e-05]\n",
      " [1.66358725e-06 9.99992756e-01 5.58003325e-06]\n",
      " [1.74488638e-05 8.29616978e-06 9.99974255e-01]]\n",
      "0.7758212597239353\n",
      "[[9.99977335e-01 7.60154149e-06 1.50636751e-05]\n",
      " [1.62966546e-06 9.99992666e-01 5.70416509e-06]\n",
      " [1.41522159e-05 9.90098600e-06 9.99975947e-01]]\n",
      "0.7758213874651696\n",
      "[[9.99977317e-01 6.49236027e-06 1.61905261e-05]\n",
      " [1.86967198e-06 9.99993241e-01 4.88930453e-06]\n",
      " [9.77679175e-06 1.11259974e-05 9.99979097e-01]]\n",
      "0.7758214163436756\n",
      "[[9.99978514e-01 9.37669559e-06 1.21093439e-05]\n",
      " [1.71180573e-06 9.99992847e-01 5.44148758e-06]\n",
      " [1.12185454e-05 1.08108116e-05 9.99977971e-01]]\n",
      "0.7758214816815716\n",
      "[[9.99977467e-01 8.10707416e-06 1.44259453e-05]\n",
      " [1.70473866e-06 9.99992833e-01 5.46236310e-06]\n",
      " [1.77766455e-06 1.29294969e-05 9.99985293e-01]]\n",
      "0.775820902147417\n",
      "[[9.99983075e-01 5.98636597e-06 1.09382070e-05]\n",
      " [2.76617813e-06 9.99995462e-01 1.77145237e-06]\n",
      " [4.97482785e-05 1.23325968e-05 9.99937919e-01]]\n",
      "0.7758212948208986\n",
      "[[9.99977970e-01 4.78640440e-06 1.72437549e-05]\n",
      " [1.70726051e-06 9.99992732e-01 5.56118392e-06]\n",
      " [1.77594365e-05 8.31525403e-06 9.99973925e-01]]\n"
     ]
    }
   ],
   "source": [
    "# execute federated policy gradient\n",
    "nb_simulations = 10\n",
    "logits = [np.random.randint(0, 10, size=(S, A)) for i in range(nb_simulations)]\n",
    "for logit_value in logits:\n",
    "    logit = deepcopy(logit_value)\n",
    "    for t in range(number_rounds):\n",
    "        logit_client_list = [deepcopy(logit) for i in range(N)]\n",
    "        for agent in range(N):\n",
    "            logit_client = deepcopy(logit_client_list[agent])\n",
    "            for lstep in range(Local_steps):\n",
    "                logit_client = logit_client + step_size * compute_gradient(agent, logit_client)\n",
    "            logit_client_list[agent] = logit_client\n",
    "        logit_avg = np.zeros((S, A))\n",
    "        for i in range(N):\n",
    "            logit_avg += logit_client_list[agent]\n",
    "        logit = logit_avg/N\n",
    "    print(compute_objective(logit))\n",
    "    print(compute_policy(logit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialisation 1\n",
      "Objective of agent 0  when solo training: 0.8346119814922899\n",
      "Objective of agent 1  when solo training: 0.8533465000653994\n",
      "Objective of agent 2  when solo training: 0.8980503591190803\n",
      "Objective of agent 3  when solo training: 0.8136753939838457\n",
      "Objective of agent 4  when solo training: 0.8188249927967213\n",
      "--------------------------------------------------------------\n",
      "Initialisation 2\n",
      "Objective of agent 0  when solo training: 0.8346119839875845\n",
      "Objective of agent 1  when solo training: 0.85334648182004\n",
      "Objective of agent 2  when solo training: 0.8980503791399955\n",
      "Objective of agent 3  when solo training: 0.8136754114213736\n",
      "Objective of agent 4  when solo training: 0.8188312056229217\n",
      "--------------------------------------------------------------\n",
      "Initialisation 3\n",
      "Objective of agent 0  when solo training: 0.8346119947152308\n",
      "Objective of agent 1  when solo training: 0.8462299020850155\n",
      "Objective of agent 2  when solo training: 0.8980503717907287\n",
      "Objective of agent 3  when solo training: 0.81367544488524\n",
      "Objective of agent 4  when solo training: 0.818831235741131\n",
      "--------------------------------------------------------------\n",
      "Initialisation 4\n",
      "Objective of agent 0  when solo training: 0.8346119791080655\n",
      "Objective of agent 1  when solo training: 0.8533464890636763\n",
      "Objective of agent 2  when solo training: 0.8980503557807573\n",
      "Objective of agent 3  when solo training: 0.8136754054174602\n",
      "Objective of agent 4  when solo training: 0.8188311990995998\n",
      "--------------------------------------------------------------\n",
      "Initialisation 5\n",
      "Objective of agent 0  when solo training: 0.8346119858871297\n",
      "Objective of agent 1  when solo training: 0.8533464867891991\n",
      "Objective of agent 2  when solo training: 0.8980503656059746\n",
      "Objective of agent 3  when solo training: 0.8136754103468467\n",
      "Objective of agent 4  when solo training: 0.8188311867845282\n",
      "--------------------------------------------------------------\n",
      "Initialisation 6\n",
      "Objective of agent 0  when solo training: 0.8346119884721397\n",
      "Objective of agent 1  when solo training: 0.8533463027485083\n",
      "Objective of agent 2  when solo training: 0.8948787863533749\n",
      "Objective of agent 3  when solo training: 0.8136753934366971\n",
      "Objective of agent 4  when solo training: 0.8188311474906117\n",
      "--------------------------------------------------------------\n",
      "Initialisation 7\n",
      "Objective of agent 0  when solo training: 0.8346119836696411\n",
      "Objective of agent 1  when solo training: 0.8533464863321536\n",
      "Objective of agent 2  when solo training: 0.8980503604742067\n",
      "Objective of agent 3  when solo training: 0.8136753911744283\n",
      "Objective of agent 4  when solo training: 0.8188307655948812\n",
      "--------------------------------------------------------------\n",
      "Initialisation 8\n",
      "Objective of agent 0  when solo training: 0.8346119988639453\n",
      "Objective of agent 1  when solo training: 0.8533464945748392\n",
      "Objective of agent 2  when solo training: 0.8980504032871914\n",
      "Objective of agent 3  when solo training: 0.8136754202306622\n",
      "Objective of agent 4  when solo training: 0.8188312122017822\n",
      "--------------------------------------------------------------\n",
      "Initialisation 9\n",
      "Objective of agent 0  when solo training: 0.834611526865162\n",
      "Objective of agent 1  when solo training: 0.8462300176630179\n",
      "Objective of agent 2  when solo training: 0.8948786559381772\n",
      "Objective of agent 3  when solo training: 0.8136753719843979\n",
      "Objective of agent 4  when solo training: 0.8188311276409994\n",
      "--------------------------------------------------------------\n",
      "Initialisation 10\n",
      "Objective of agent 0  when solo training: 0.8346119785051839\n",
      "Objective of agent 1  when solo training: 0.8533464765422115\n",
      "Objective of agent 2  when solo training: 0.898050365320314\n",
      "Objective of agent 3  when solo training: 0.8136753991427244\n",
      "Objective of agent 4  when solo training: 0.8188311766265282\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# execute policy gradient without aggregation\n",
    "nb_simulations = 10\n",
    "logits = [np.random.randint(0, 10, size=(S, A)) for i in range(nb_simulations)]\n",
    "Initialisation = 0\n",
    "for logit_value in logits:\n",
    "    Initialisation +=1\n",
    "    logit_client_list = [deepcopy(logit_value) for i in range(N)]\n",
    "    for t in range(50*number_rounds):\n",
    "        for agent in range(N):\n",
    "            logit_client = deepcopy(logit_client_list[agent])\n",
    "            logit_client = logit_client + step_size * compute_gradient(agent, logit_client)\n",
    "            logit_client_list[agent] = logit_client\n",
    "    print(\"Initialisation\",Initialisation )\n",
    "    for agent in range(N):\n",
    "        print(\"Objective of agent\", agent,\" when solo training:\", compute_objective_agent_i(logit_client_list[agent],agent))\n",
    "    print(\"--------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt to find counter-examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Params\n",
    "np.random.seed(1)\n",
    "N=2\n",
    "S, A = 3, 2 # state space\n",
    "gamma = 0.9 #discount factor \n",
    "init_dist = np.array([0,0,1]) #init_distribution\n",
    "number_rounds = 50000\n",
    "Local_steps = 1\n",
    "step_size  = 0.001\n",
    "tau = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate environnements\n",
    "env1 = FiniteStateFiniteActionMDP(S=3, A=2)\n",
    "env2 = FiniteStateFiniteActionMDP(S=3, A=2)\n",
    "P_1 = env1.get_P()\n",
    "r = env1.get_r()\n",
    "P_1[0,0,0] = 1.0\n",
    "P_1[0,0,1] = 0.0\n",
    "P_1[0,0,2] = 0.0\n",
    "P_1[0,1,0] = 1.0\n",
    "P_1[0,1,1] = 0.0\n",
    "P_1[0,1,2] = 0.0\n",
    "\n",
    "P_1[1,0,0] = 0.0\n",
    "P_1[1,0,1] = 1.0\n",
    "P_1[1,0,2] = 0.0\n",
    "P_1[1,1,0] = 0.0\n",
    "P_1[1,1,1] = 1.0\n",
    "P_1[1,1,2] = 0.0\n",
    "\n",
    "\n",
    "P_1[2,0,0] = 0.0\n",
    "P_1[2,0,1] = 0.0\n",
    "P_1[2,0,2] = 1.0\n",
    "P_1[2,1,0] = 0.0\n",
    "P_1[2,1,1] = 1.0\n",
    "P_1[2,1,2] = 0.0\n",
    "\n",
    "\n",
    "#Second MDP\n",
    "P_2 = env2.get_P()\n",
    "\n",
    "P_2[0,0,0] = 1.0\n",
    "P_2[0,0,1] = 0.0\n",
    "P_2[0,0,2] = 0.0\n",
    "P_2[0,1,0] = 1.0\n",
    "P_2[0,1,1] = 0.0\n",
    "P_2[0,1,2] = 0.0\n",
    "\n",
    "P_2[1,0,0] = 0.0\n",
    "P_2[1,0,1] = 1.0\n",
    "P_2[1,0,2] = 0.0\n",
    "P_2[1,1,0] = 0.0\n",
    "P_2[1,1,1] = 1.0\n",
    "P_2[1,1,2] = 0.0\n",
    "\n",
    "\n",
    "P_2[2,0,0] = 1.0\n",
    "P_2[2,0,1] = 0.0\n",
    "P_2[2,0,2] = 0.0\n",
    "P_2[2,1,0] = 0.0\n",
    "P_2[2,1,1] = 0.0\n",
    "P_2[2,1,2] = 1.0\n",
    "\n",
    "r[0,0] = 10\n",
    "r[0,1] = 10\n",
    "r[1,0] = 10\n",
    "r[1,1] = 10\n",
    "r[2,0] = 0.0\n",
    "r[2,1] = 0.0\n",
    "\n",
    "env1.P = P_1\n",
    "env2.P = P_2\n",
    "env1.R = r\n",
    "env2.R = r\n",
    "envs = [env1, env2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Useful functions\n",
    "\n",
    "def compute_mrp_transition(agent, policy):\n",
    "    transition_kernel =  envs[agent].get_P()\n",
    "    mrp_transition = np.sum(policy[:, :, np.newaxis] * transition_kernel, axis=1)\n",
    "    return mrp_transition\n",
    "\n",
    "def compute_mrp_reward(agent, policy):\n",
    "    reward = envs[agent].get_r()\n",
    "    # Element-wise multiplication and sum along the actions axis\n",
    "    mrp_reward = np.sum(policy * reward, axis=1)\n",
    "    return mrp_reward\n",
    "\n",
    "def compute_stationnary_distribution(agent, policy):\n",
    "    mrp_transition = compute_mrp_transition(agent, policy)\n",
    "    stationnary_distribution = (1- gamma) *init_dist.T @ np.linalg.inv(np.eye(S) -gamma *mrp_transition) \n",
    "    return stationnary_distribution\n",
    "\n",
    "def compute_value_function(agent, policy):\n",
    "    mrp_transition = compute_mrp_transition(agent, policy)\n",
    "    mrp_reward = compute_mrp_reward(agent, policy)\n",
    "    return np.linalg.inv(np.eye(S) -gamma *mrp_transition) @ mrp_reward\n",
    "\n",
    "def compute_qfunction(agent, policy):\n",
    "    reward = envs[agent].get_r()\n",
    "    transitions = envs[agent].get_P()\n",
    "    value_function = compute_value_function(agent, policy)\n",
    "    expected_future_rewards = np.sum(transitions * value_function[np.newaxis, np.newaxis, :], axis=2)\n",
    "    Q_function = reward + gamma * expected_future_rewards\n",
    "    return Q_function\n",
    "\n",
    "def compute_policy(logits):\n",
    "    logits_max = np.max(logits, axis=1, keepdims=True)\n",
    "    exp_logits = np.exp(logits - logits_max)\n",
    "    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "def compute_gradient(agent, logits):\n",
    "    policy = compute_policy(logits)\n",
    "    stationary_distribution = compute_stationnary_distribution(agent, policy)\n",
    "    stationary_distribution = stationary_distribution[:, np.newaxis]\n",
    "    print(stationary_distribution)\n",
    "    q_function = compute_qfunction(agent, policy)\n",
    "    value_function = compute_value_function(agent, policy)\n",
    "    advantage = q_function - value_function[:, None]\n",
    "    gradient = (1 / (1 - gamma)) * stationary_distribution * policy * advantage\n",
    "    return gradient\n",
    "\n",
    "def compute_objective(logits):\n",
    "    policy = compute_policy(logits)\n",
    "    objective = 0.0\n",
    "    for agent in range(N):\n",
    "        statinnary_distrubtion_agent = compute_stationnary_distribution(agent, policy)\n",
    "        reward_mrp = compute_mrp_reward(agent, policy)\n",
    "        objective += np.dot(statinnary_distrubtion_agent,reward_mrp)\n",
    "    return(objective/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 6.00000000e+00,  2.00000000e+00],\n",
      "       [ 9.00000000e+00,  5.00000000e+00],\n",
      "       [-7.43801653e-03,  7.43801653e-03]]), array([[ 6.00000000e+00,  2.00000000e+00],\n",
      "       [ 9.00000000e+00,  5.00000000e+00],\n",
      "       [ 7.43801653e-03, -7.43801653e-03]])]\n",
      "objective 8.181750845199659\n",
      "policy [[0.98201379 0.01798621]\n",
      " [0.98201379 0.01798621]\n",
      " [0.50371894 0.49628106]]\n",
      "[array([[ 6.00000000e+00,  2.00000000e+00],\n",
      "       [ 9.00000000e+00,  5.00000000e+00],\n",
      "       [-9.09452488e-05,  9.09452488e-05]]), array([[ 6.        ,  2.        ],\n",
      "       [ 9.        ,  5.        ],\n",
      "       [ 0.01478592, -0.01478592]])]\n",
      "objective 8.181552088614652\n",
      "policy [[0.98201379 0.01798621]\n",
      " [0.98201379 0.01798621]\n",
      " [0.50739242 0.49260758]]\n",
      "[array([[ 6.00000000e+00,  2.00000000e+00],\n",
      "       [ 9.00000000e+00,  5.00000000e+00],\n",
      "       [ 7.16629761e-03, -7.16629761e-03]]), array([[ 6.        ,  2.        ],\n",
      "       [ 9.        ,  5.        ],\n",
      "       [ 0.02204561, -0.02204561]])]\n",
      "objective 8.181226645037288\n",
      "policy [[0.98201379 0.01798621]\n",
      " [0.98201379 0.01798621]\n",
      " [0.51102102 0.48897898]]\n",
      "[array([[ 6.        ,  2.        ],\n",
      "       [ 9.        ,  5.        ],\n",
      "       [ 0.01433562, -0.01433562]]), array([[ 6.        ,  2.        ],\n",
      "       [ 9.        ,  5.        ],\n",
      "       [ 0.02921895, -0.02921895]])]\n",
      "objective 8.180779058621425\n",
      "policy [[0.98201379 0.01798621]\n",
      " [0.98201379 0.01798621]\n",
      " [0.51460532 0.48539468]]\n"
     ]
    }
   ],
   "source": [
    "# execute policy gradient\n",
    "nb_simulations = 1\n",
    "logits = [np.random.randint(0, 10, size=(S, A)) for i in range(nb_simulations)]\n",
    "logits[0][2] = [0.5,0.5]\n",
    "gradient = np.zeros((S, A))\n",
    "for agent in range(N):\n",
    "    gradient +=compute_gradient(agent, logits[0])\n",
    "#print(\"norm of the gradient at the beggining of the training\", np.linalg.norm(gradient))\n",
    "for logit_value in logits:\n",
    "    logit = deepcopy(logit_value)\n",
    "    for t in range(4):\n",
    "        logit_client_list = [deepcopy(logit) for i in range(N)]\n",
    "        gradient = np.zeros((S, A))\n",
    "        for agent in range(N):\n",
    "            logit_client = deepcopy(logit_client_list[agent])\n",
    "            for lstep in range(Local_steps):\n",
    "                gradient_agent = compute_gradient(agent, logit_client)\n",
    "                gradient += gradient_agent\n",
    "                logit_client = logit_client + step_size * gradient_agent\n",
    "            logit_client_list[agent] = logit_client\n",
    "        #print(\"gradient\",gradient)\n",
    "        #print(\"norm of the gradient\", np.linalg.norm(gradient))\n",
    "        print(logit_client_list)\n",
    "        logit_avg = np.zeros((S, A))\n",
    "        for i in range(N):\n",
    "            logit_avg += logit_client_list[agent]\n",
    "        logit = logit_avg/N\n",
    "        print(\"objective\", compute_objective(logit))\n",
    "        print(\"policy\",compute_policy(logit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00  0.00000000e+00]\n",
      " [ 1.42108547e-14  1.42108547e-14]\n",
      " [-8.18181818e+00  8.18181818e+00]]\n",
      "[[ 0.00000000e+00  0.00000000e+00]\n",
      " [ 1.42108547e-14  1.42108547e-14]\n",
      " [ 8.18181818e+00 -8.18181818e+00]]\n",
      "[[1.  0.  0. ]\n",
      " [0.  1.  0. ]\n",
      " [0.5 0.  0.5]]\n",
      "[0.81818182 0.         0.18181818]\n"
     ]
    }
   ],
   "source": [
    "nb_simulations = 1\n",
    "logits = [np.random.randint(0, 10, size=(S, A)) for i in range(nb_simulations)]\n",
    "logits[0][2] = [1,1]\n",
    "policy = compute_policy(logits[0])\n",
    "stat_dist = compute_stationnary_distribution(1, compute_policy(logits[0]))\n",
    "q_function_1 = compute_qfunction(0, policy)\n",
    "value_function_1 = compute_value_function(0, policy)\n",
    "q_function_2 = compute_qfunction(1, policy)\n",
    "value_function_2 = compute_value_function(1, policy)\n",
    "advantage1 = q_function_1 - value_function_1[:, None]\n",
    "advantage2 = q_function_2 - value_function_2[:, None]\n",
    "print(advantage1)\n",
    "print(advantage2)\n",
    "print(compute_mrp_transition(1, compute_policy(logits[0])))\n",
    "print(stat_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_simulations = 20\n",
    "logits = [np.random.randint(0, 10, size=(S, A)) for i in range(nb_simulations)]\n",
    "logit = deepcopy(logits[11])\n",
    "for t in range(10*number_rounds):\n",
    "    logit_client_list = [deepcopy(logit) for i in range(N)]\n",
    "    for agent in range(N):\n",
    "        logit_client = deepcopy(logit_client_list[agent])\n",
    "        for lstep in range(Local_steps):\n",
    "            logit_client = logit_client + step_size * compute_gradient(agent, logit_client)\n",
    "        logit_client_list[agent] = logit_client\n",
    "    logit_avg = np.zeros((S, A))\n",
    "    for i in range(N):\n",
    "        logit_avg += logit_client_list[agent]\n",
    "    logit = logit_avg/N\n",
    "print(compute_objective(logit))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
